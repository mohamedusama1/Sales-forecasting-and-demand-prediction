{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93992807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data shape: (3002670, 84)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings \n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Show all columns and full width\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Helper feature engineering functions ---\n",
    "\n",
    "def create_date_features(df):\n",
    "    df['month'] = df.date.dt.month.astype('int8')\n",
    "    df['day_of_month'] = df.date.dt.day.astype('int8')\n",
    "    df['day_of_year'] = df.date.dt.dayofyear.astype('int16')\n",
    "    df['week_of_month'] = ((df.date.dt.day - 1) // 7 + 1).astype('int8')\n",
    "    df['week_of_year'] = df.date.dt.isocalendar().week.astype('int8')\n",
    "    df['day_of_week'] = (df.date.dt.dayofweek + 1).astype('int8')\n",
    "    df['year'] = df.date.dt.year.astype('int32')\n",
    "    df['is_wknd'] = (df.date.dt.weekday // 4).astype('int8')\n",
    "    df['quarter'] = df.date.dt.quarter.astype('int8')\n",
    "    df['is_month_start'] = df.date.dt.is_month_start.astype('int8')\n",
    "    df['is_month_end'] = df.date.dt.is_month_end.astype('int8')\n",
    "    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype('int8')\n",
    "    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype('int8')\n",
    "    df['is_year_start'] = df.date.dt.is_year_start.astype('int8')\n",
    "    df['is_year_end'] = df.date.dt.is_year_end.astype('int8')\n",
    "    df['season'] = np.where(df.month.isin([12,1,2]), 0,\n",
    "                             np.where(df.month.isin([6,7,8]), 2,\n",
    "                                      np.where(df.month.isin([9,10,11]), 3, 1))).astype('int8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original = list(df.columns)\n",
    "    cats = df.select_dtypes(['object','category']).columns.tolist()\n",
    "    df_enc = pd.get_dummies(df, columns=cats, dummy_na=nan_as_category)\n",
    "    df_enc.columns = df_enc.columns.str.replace(' ', '_')\n",
    "    new_cols = [c for c in df_enc.columns if c not in original]\n",
    "    return df_enc, new_cols\n",
    "\n",
    "\n",
    "def create_oil_features(df, oil):\n",
    "    \"\"\"\n",
    "    Create oil-related features from the oil price data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Main dataframe\n",
    "    oil : pandas.DataFrame\n",
    "        Oil price dataframe with 'date' and 'dcoilwtico' columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with added oil features\n",
    "    \"\"\"\n",
    "    # Parse date if not already done\n",
    "    if not pd.api.types.is_datetime64_dtype(oil['date']):\n",
    "        oil['date'] = pd.to_datetime(oil['date'])\n",
    "        \n",
    "    # Interpolate oil price\n",
    "    oil_series = (\n",
    "        oil.set_index('date')['dcoilwtico']\n",
    "           .replace(0, np.nan)\n",
    "           .interpolate()\n",
    "           .fillna(method='bfill')\n",
    "           .rename('dcoilwtico_interpolated')\n",
    "           .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Merge with main dataframe\n",
    "    df = df.merge(oil_series, on='date', how='left')\n",
    "    \n",
    "    # Create binary feature for high oil price\n",
    "    df['oil_above_70'] = (df['dcoilwtico_interpolated'] >= 70).astype('int8')\n",
    "    \n",
    "    # Drop intermediate column if not needed\n",
    "    df.drop('dcoilwtico_interpolated', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_holiday_features(df, holidays):\n",
    "    \"\"\"\n",
    "    Create holiday-related features from the holidays data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Main dataframe\n",
    "    holidays : pandas.DataFrame\n",
    "        Holidays dataframe with required columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with added holiday features and work_day dataframe\n",
    "    \"\"\"\n",
    "    # Parse date if not already done\n",
    "    holidays[\"date\"] = pd.to_datetime(holidays.date)\n",
    "\n",
    "    \n",
    "    # Ensure description is string type\n",
    "    holidays['description'] = holidays['description'].fillna('').astype(str)\n",
    "    \n",
    "    # Fix \"transferred\" column if it's not a boolean\n",
    "    if not pd.api.types.is_bool_dtype(holidays['transferred']):\n",
    "        holidays['transferred'] = holidays['transferred'].astype(bool)\n",
    "    \n",
    "    # Transferred Holidays\n",
    "    tr1 = holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)].drop(\"transferred\", axis=1).reset_index(drop=True)\n",
    "    tr2 = holidays[(holidays.type == \"Transfer\")].drop(\"transferred\", axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Check if there are any rows before trying to concatenate\n",
    "    if len(tr1) > 0 and len(tr2) > 0:\n",
    "        tr = pd.concat([tr1, tr2], axis=1)\n",
    "        tr = tr.iloc[:, [5,1,2,3,4]] if tr.shape[1] > 5 else tr  # Ensure proper indexing\n",
    "    else:\n",
    "        # Create an empty DataFrame with the same columns\n",
    "        tr = pd.DataFrame(columns=holidays.columns)\n",
    "    \n",
    "    holidays = holidays[(holidays.transferred == False) & (holidays.type != \"Transfer\")].drop(\"transferred\", axis=1)\n",
    "    \n",
    "    # Only concatenate if tr is not empty\n",
    "    if not tr.empty:\n",
    "        holidays = pd.concat([holidays, tr], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Additional Holidays - safely apply string operations\n",
    "    holidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\", regex=False).str.replace(\"+\", \"\", regex=False).str.replace(r\"\\d+\", \"\", regex=True)\n",
    "    holidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n",
    "\n",
    "    # Bridge Holidays\n",
    "    holidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\", regex=False)\n",
    "    holidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])\n",
    "\n",
    "    # Work Day Holidays, that is meant to payback the Bridge\n",
    "    work_day = holidays[holidays.type == \"Work Day\"]\n",
    "    holidays = holidays[holidays.type != \"Work Day\"]\n",
    "\n",
    "    # Split\n",
    "    # Events are national\n",
    "    events = holidays[holidays.type == \"Event\"].drop([\"type\", \"locale\", \"locale_name\"], axis=1).rename({\"description\":\"events\"}, axis=1)\n",
    "\n",
    "    holidays = holidays[holidays.type != \"Event\"].drop(\"type\", axis=1)\n",
    "    \n",
    "    # Only proceed if there are rows in holidays\n",
    "    if not holidays.empty:\n",
    "        regional = holidays[holidays.locale == \"Regional\"].rename({\"locale_name\":\"state\", \"description\":\"holiday_regional\"}, axis=1).drop(\"locale\", axis=1).drop_duplicates()\n",
    "        national = holidays[holidays.locale == \"National\"].rename({\"description\":\"holiday_national\"}, axis=1).drop([\"locale\", \"locale_name\"], axis=1).drop_duplicates()\n",
    "        local = holidays[holidays.locale == \"Local\"].rename({\"description\":\"holiday_local\", \"locale_name\":\"city\"}, axis=1).drop(\"locale\", axis=1).drop_duplicates()\n",
    "    else:\n",
    "        # Create empty DataFrames with the correct columns\n",
    "        regional = pd.DataFrame(columns=[\"date\", \"state\", \"holiday_regional\"])\n",
    "        national = pd.DataFrame(columns=[\"date\", \"holiday_national\"])\n",
    "        local = pd.DataFrame(columns=[\"date\", \"city\", \"holiday_local\"])\n",
    "\n",
    "    # Merge National Holidays\n",
    "    df = df.merge(national, how=\"left\", on=\"date\")\n",
    "    \n",
    "    # Regional - ensure 'state' column exists in both DataFrames\n",
    "    if 'state' in df.columns:\n",
    "        df = df.merge(regional, how=\"left\", on=[\"date\", \"state\"])\n",
    "    else:\n",
    "        # Add empty holiday_regional column if state column doesn't exist\n",
    "        df['holiday_regional'] = np.nan\n",
    "        \n",
    "    # Local - ensure 'city' column exists in both DataFrames\n",
    "    if 'city' in df.columns:\n",
    "        df = df.merge(local, how=\"left\", on=[\"date\", \"city\"])\n",
    "    else:\n",
    "        # Add empty holiday_local column if city column doesn't exist\n",
    "        df['holiday_local'] = np.nan\n",
    "\n",
    "    # Work Day\n",
    "    if not work_day.empty:\n",
    "        df = df.merge(work_day[[\"date\", \"type\"]].rename({\"type\":\"IsWorkDay\"}, axis=1), how=\"left\")\n",
    "    else:\n",
    "        df[\"IsWorkDay\"] = np.nan\n",
    "    \n",
    "    # EVENTS\n",
    "    # Ensure events column is string type before using str methods\n",
    "    if 'events' in events.columns and not events.empty:\n",
    "        events['events'] = events['events'].fillna('').astype(str)\n",
    "        events[\"events\"] = np.where(events.events.str.contains(\"futbol\", na=False), \"Futbol\", events.events)\n",
    "        \n",
    "        # One-hot encode events\n",
    "        events_enc, events_cat = one_hot_encoder(events)\n",
    "        \n",
    "        # Special case for Mother's Day\n",
    "        if 'events_Dia_de_la_Madre' in events_enc.columns and len(events_enc) > 239:\n",
    "            mother_day_date = pd.to_datetime(\"2016-05-08\")\n",
    "            events_enc.loc[events_enc.date == mother_day_date, 'events_Dia_de_la_Madre'] = 1\n",
    "            if 239 < len(events_enc):\n",
    "                events_enc = events_enc.drop(239)\n",
    "        \n",
    "        df = df.merge(events_enc, how=\"left\", on=\"date\")\n",
    "        if events_cat:  # Only try to fill if the list is not empty\n",
    "            df[events_cat] = df[events_cat].fillna(0)\n",
    "    \n",
    "    # New features - safely handle possibly missing columns\n",
    "    if 'holiday_national' in df.columns:\n",
    "        df[\"holiday_national_binary\"] = np.where(df.holiday_national.notna(), 1, 0).astype('int8')\n",
    "    else:\n",
    "        df[\"holiday_national_binary\"] = 0\n",
    "        \n",
    "    if 'holiday_local' in df.columns:\n",
    "        df[\"holiday_local_binary\"] = np.where(df.holiday_local.notna(), 1, 0).astype('int8')\n",
    "    else:\n",
    "        df[\"holiday_local_binary\"] = 0\n",
    "        \n",
    "    if 'holiday_regional' in df.columns:\n",
    "        df[\"holiday_regional_binary\"] = np.where(df.holiday_regional.notna(), 1, 0).astype('int8')\n",
    "    else:\n",
    "        df[\"holiday_regional_binary\"] = 0\n",
    "    \n",
    "    # Additional holiday features\n",
    "    if 'holiday_national' in df.columns:\n",
    "        df[\"national_independence\"] = np.where(\n",
    "            df.holiday_national.isin([\n",
    "                'Batalla de Pichincha', 'Independencia de Cuenca', \n",
    "                'Independencia de Guayaquil', 'Primer Grito de Independencia'\n",
    "            ]), \n",
    "            1, 0\n",
    "        ).astype('int8')\n",
    "    else:\n",
    "        df[\"national_independence\"] = 0\n",
    "    \n",
    "    # Process local holiday features if they exist\n",
    "    if 'holiday_local' in df.columns:\n",
    "        # First ensure column is string type and fill NaN values\n",
    "        df['holiday_local'] = df['holiday_local'].fillna('').astype(str)\n",
    "        \n",
    "        # Now safely apply string methods\n",
    "        df[\"local_cantonizacio\"] = np.where(df.holiday_local.str.contains(\"Cantonizacio\", na=False), 1, 0).astype('int8')\n",
    "        df[\"local_fundacion\"] = np.where(df.holiday_local.str.contains(\"Fundacion\", na=False), 1, 0).astype('int8')\n",
    "        df[\"local_independencia\"] = np.where(df.holiday_local.str.contains(\"Independencia\", na=False), 1, 0).astype('int8')\n",
    "    else:\n",
    "        df[\"local_cantonizacio\"] = 0\n",
    "        df[\"local_fundacion\"] = 0\n",
    "        df[\"local_independencia\"] = 0\n",
    "    \n",
    "    # One-hot encode holiday columns if they exist\n",
    "    holiday_cols = [\"holiday_national\", \"holiday_regional\", \"holiday_local\"]\n",
    "    existing_holiday_cols = [col for col in holiday_cols if col in df.columns]\n",
    "    \n",
    "    if existing_holiday_cols:\n",
    "        for col in existing_holiday_cols:\n",
    "            # Ensure values are strings before one-hot encoding\n",
    "            df[col] = df[col].fillna('').astype(str)\n",
    "            \n",
    "        # Now it's safe to one-hot encode\n",
    "        holidays_enc, holidays_cat = one_hot_encoder(df[existing_holiday_cols], nan_as_category=False)\n",
    "        df = pd.concat([df.drop(existing_holiday_cols, axis=1), holidays_enc], axis=1)\n",
    "    \n",
    "    # Convert holiday columns to int8\n",
    "    he_cols = (\n",
    "        df.columns[df.columns.str.startswith(\"events\")].tolist() + \n",
    "        df.columns[df.columns.str.startswith(\"holiday\")].tolist() + \n",
    "        df.columns[df.columns.str.startswith(\"national\")].tolist() + \n",
    "        df.columns[df.columns.str.startswith(\"local\")].tolist()\n",
    "    )\n",
    "    \n",
    "    # Only convert columns that exist and contain numeric data\n",
    "    existing_he_cols = [col for col in he_cols if col in df.columns]\n",
    "    if existing_he_cols:\n",
    "        for col in existing_he_cols:\n",
    "            try:\n",
    "                df[col] = df[col].fillna(0).astype(\"int8\")\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion fails, leave as is\n",
    "                pass\n",
    "    \n",
    "    return df, work_day\n",
    "\n",
    "\n",
    "# --- Core preprocessing pipeline ---\n",
    "\n",
    "def preprocess_full(\n",
    "    raw: pd.DataFrame,\n",
    "    stores: pd.DataFrame,\n",
    "    transactions: pd.DataFrame,\n",
    "    oil: pd.DataFrame,\n",
    "    holidays: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # parse dates\n",
    "    raw['date'] = pd.to_datetime(raw['date'])\n",
    "    \n",
    "    # merge store info\n",
    "    df = raw.merge(stores, on='store_nbr', how='left')\n",
    "\n",
    "    df, work_day = create_holiday_features(df, holidays)\n",
    "    df = create_date_features(df)\n",
    "\n",
    "    # add oil features\n",
    "    df = create_oil_features(df, oil)\n",
    "    \n",
    "    # date features first (needed for workday calculation)\n",
    "    \n",
    "    # add holiday features\n",
    "\n",
    "    # flags\n",
    "    df['workday'] = (~(\n",
    "        (df.holiday_national_binary==1) |\n",
    "        (df.holiday_regional_binary==1) |\n",
    "        (df.holiday_local_binary==1) |\n",
    "        (df.day_of_week.isin([6,7]))\n",
    "    )).astype('int8')\n",
    "    \n",
    "    # Safely handle work_day\n",
    "    if not work_day.empty and 'date' in work_day.columns:\n",
    "        df.loc[df.date.isin(work_day.date), 'workday'] = 1\n",
    "        \n",
    "    df['wageday'] = ((df.is_month_end==1)|(df.day_of_month==15)).astype('int8')\n",
    "\n",
    "    # categories\n",
    "    for c in ['family','city','state','type','cluster','oil_above_70']:\n",
    "        if c in df.columns:  # Only try to convert if the column exists\n",
    "            df[c] = df[c].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Feature inspection interface ---\n",
    "\n",
    "def inspect_features_from_dicts(\n",
    "    input_data: dict,\n",
    "    stores_data: dict,\n",
    "    transactions_data: dict,\n",
    "    oil_data: dict,\n",
    "    holidays_data: dict\n",
    ") -> pd.DataFrame:\n",
    "    norm = {}\n",
    "    for k, v in input_data.items():\n",
    "        norm[k] = v if isinstance(v, (list, np.ndarray)) else [v]\n",
    "    df_raw = pd.DataFrame(norm)\n",
    "\n",
    "    stores = pd.DataFrame(stores_data)\n",
    "    transactions = pd.DataFrame(transactions_data)\n",
    "    oil = pd.DataFrame(oil_data)\n",
    "    holidays = pd.DataFrame(holidays_data)\n",
    "\n",
    "    df_feat = preprocess_full(df_raw, stores, transactions, oil, holidays)\n",
    "\n",
    "    print(\"=== Engineered Feature Sample ===\")\n",
    "    display(df_feat.head())\n",
    "    return df_feat\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     example_input = {\n",
    "#         'id': 3000888,\n",
    "#         'date': '2017-08-16',\n",
    "#         'store_nbr': 1,\n",
    "#         'family': 'AUTOMOTIVE',\n",
    "#         'onpromotion': 0\n",
    "#     }\n",
    "#     stores_dict = pd.read_csv('stores.csv')\n",
    "#     transactions_dict = pd.read_csv('transactions.csv')\n",
    "#     oil_dict = pd.read_csv('oil.csv')\n",
    "#     holidays_dict = pd.read_csv('holidays_events.csv')\n",
    "\n",
    "#     _ = inspect_features_from_dicts(\n",
    "#         example_input,\n",
    "#         stores_dict,\n",
    "#         transactions_dict,\n",
    "#         oil_dict,\n",
    "#         holidays_dict\n",
    "#     )\n",
    "if __name__ == '__main__':\n",
    "    # 1) Load all CSVs\n",
    "    raw       = pd.read_csv('train.csv',  parse_dates=['date'])\n",
    "    stores    = pd.read_csv('stores.csv')\n",
    "    transactions = pd.read_csv('transactions.csv', parse_dates=['date'])\n",
    "    oil       = pd.read_csv('oil.csv',      parse_dates=['date'])\n",
    "    holidays  = pd.read_csv('holidays_events.csv')\n",
    "\n",
    "    # 2) Preprocess the entire dataset\n",
    "    df = preprocess_full(\n",
    "        raw=raw,\n",
    "        stores=stores,\n",
    "        transactions=transactions,\n",
    "        oil=oil,\n",
    "        holidays=holidays\n",
    "    )\n",
    "\n",
    "    # 3) (Optional) Save to disk for later training\n",
    "    df.to_csv('train_preprocessed.csv', index=False)\n",
    "    print(f'Preprocessed data shape: {df.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5af7c4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4385e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 4) Save the list of all columns seen in training\n",
    "feature_cols = df.columns.tolist()\n",
    "with open('feature_columns.json', 'w') as f:\n",
    "    json.dump(feature_cols, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42e13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
